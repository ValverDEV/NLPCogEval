{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset.ipynb","provenance":[],"authorship_tag":"ABX9TyOz7xxMYnLL2jWxJzEYM7ul"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6-final"}},"cells":[{"source":["# Script para generar un dataset (pelota de plata)\n","\n","Se pretende generar un dataset compatible con la librería datasets para utilizarlo en transformers. Este script es único para pelota de plata.\n","\n","Primero importamos las librerías."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"f2_pK489dWM_"},"source":["import datasets\n","import json"],"execution_count":1,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'datasets' has no attribute 'logging'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-ba62a38bc756>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\mv502\\Desktop\\NLPCogEval\\Aprendiendo\\datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0m_CITATION\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# ## Clases que permitirán al script ser cargado desde otro archivo usando _datasets.load_dataset()_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: module 'datasets' has no attribute 'logging'"]}]},{"source":["#### Establecemos la metadata del dataset"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"C9gBuUuWe7-r"},"source":["_DESCRIPTION = 'CSV secundaria para NLP'\n","_ULR = 'https://github.com/ValverDEV/NLPCogEval/blob/main/datasets/JSON/pelota_plata.json?raw=true'\n","_CITATION = ''\n","\n","logger = datasets.logging.get_logger(__name__)"],"execution_count":3,"outputs":[]},{"source":["## Clases que permitirán al script ser cargado desde otro archivo usando _datasets.load_dataset()_"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"l3LPZSqzdVCw"},"source":["class NLPConfig(datasets.BuilderConfig):\n","\n","  def __init__(self, **kwargs):\n","\n","    super(NLPConfig, self).__init__(**kwargs)\n","\n","\n","class NLP(datasets.GeneratorBasedBuilder):\n","\n","  BuilderConfig = [\n","    NLPConfig(\n","      name='plain_text',\n","      version=datasets.Version('1.0.0',''),\n","      description='Plain text',\n","    ),\n","  ]\n","\n","  def _info(self):\n","    return datasets.DatasetInfo(\n","        description = _DESCRIPTION,\n","        features = datasets.Features(\n","            {\n","                'answer' : datasets.Value('string'),\n","                'label' : datasets.Value('bool')\n","            }\n","        ),\n","    citation=_CITATION\n","    )\n","\n","\n","    def _split_generators(self, dl_manager):\n","      downloaded_files = dl_manager.download_and_extract(_URL)\n","\n","      return [\n","              datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={'filepath': downloaded_files['train']}),\n","              datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={'filepath': downloaded_files['dev']})\n","      ]\n","\n","    def _generate_examples(self, filepath):\n","      logger.info(f'generating examples from, {filepath}')\n","      with open(filepath, encoding='latin1') as f:\n","        datos = json.load(f)\n","        for respuesta in datos:\n","          answer = respuesta['answer']\n","          label = respuesta['label']\n","\n","          yield id_, {\n","              'answer' : answer,\n","              'label' : label\n","          }"],"execution_count":6,"outputs":[]}]}